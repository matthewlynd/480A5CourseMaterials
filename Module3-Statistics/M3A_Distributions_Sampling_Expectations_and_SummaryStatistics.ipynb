{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <summary></summary>\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\"> \n",
    "              <h1>\n",
    "               <left>Intro to Basic Probability and Statistics: Distributions, Sampling, and Expectations.</left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>\n",
    "<pre>Course: BIOM 480A5, Spring 2025\n",
    "Instructor: Brian Munsky\n",
    "Authors: Huy Vo, Ania Baetica, Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2024 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learning Objectives**\n",
    "![alt text](figuresA/image001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following the first time if you do not have sympy or seaborn installed:\n",
    "# !pip install sympy\n",
    "# !pip install seaborn\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sympy as sym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from typing import Union # for code annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the user is usig the currect working directory\n",
    "import os\n",
    "path = os.getcwd()\n",
    "# Get the folder name\n",
    "folder = os.path.basename(path)\n",
    "\n",
    "print('Checking current directory...')\n",
    "if folder == 'Module3-Statistics':\n",
    "    print(f'You are currently in the correct directory: {folder}')\n",
    "elif folder == '480A5CourseMaterials':\n",
    "    os.chdir('Module3-Statistics')\n",
    "    print('Changed to Module3-Statistics directory')\n",
    "else:\n",
    "    print('You are in the wrong directory. Navigate to the Module3-Statistics directory and try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Random Numbers**\n",
    "![alt text](figuresA/image002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random variable is a quantity $X$ whose precise value is _uncertain_. For example, the number of cells in the next microscope field of view you image or the height of the first person you see on the street tomorrow are random variables.\n",
    "\n",
    "If $X$ can only take integer values $0,1,2,\\ldots$ we say that $X$ is a __discrete random variable__. If $X$ can take the whole range of real values in $\\mathbb{R}$, we say that $X$ is a __continuous random variable__. There are random variables that can behave discretely in some domain and continuously in others, but for the purpose of the summer school we will only focus on these two types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Probability Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.A. Probability Mass Functions and Probability Density Functions**\n",
    "![alt text](figuresA/image003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $X$ is discrete, the __probability mass function__ (PMF) of $X$ gives us the probability for it to take on each possible specific value $n=0,1,2,\\ldots$:\n",
    "$$\n",
    "p_{X}(n) = \\mathrm{Pr}\\left(X = n\\right).\n",
    "$$\n",
    "\n",
    "When $X$ is continuous, we are interested in the __probability density function__ (PDF) of $X$, defined as a function $f_X(x)$ taking nonnegative values such that\n",
    "$$\n",
    "\\int_a^b{f_X(x)\\mathrm{dx}} = \\mathrm{Pr}(a \\leq X \\leq b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.B. Cumulative Probability Distributions**\n",
    "![alt text](figuresA/image004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Common Probability Distributions**\n",
    "## **3.A. Common Discrete Distrbutions**\n",
    "There are several discrete distributions that we will encounter in this class.  The most important are Bernoulli, Binomial, and Poisson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.1. Bernoulli Distribution**\n",
    "![alt text](figuresA/image011B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is used to model a binary reponse $Y$ that, without loss of generality, we assume to be either $0$ or $1$. Its probability mass function is given by\n",
    "$$\n",
    "\\mathrm{Ber}\\left(y\\vert \\theta\\right)\n",
    ":=\n",
    "\\begin{cases}\n",
    "1-\\theta \\;\\;\\text{if }y=0\\\\\n",
    "\\theta \\;\\;\\text{if }y=1\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\theta$ is the parameter of the distribution. This parameter has the clear interpretation as the probability for $Y$ to take value $1$. You can use this distribution to model a _single occurence_ of a trial that has binary outcomes, such as the a single coin flip.\n",
    "\n",
    "The Bernoulli distribution is important for understanding binary classification (e.g., in machine learning).\n",
    "\n",
    "* What is the mean of a Bernoulli random variable?\n",
    "\n",
    "* What is it's standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Bernoulli random variables\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "theta = 0.7  # probability of success - change this value to see the effect\n",
    "\n",
    "# Create a Bernoulli random variable\n",
    "ber_rv = bernoulli(p = theta)\n",
    "\n",
    "# Make two plots to show the PMF and the CDF:\n",
    "# The first one is a bar chart of the probability mass function, \n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "bplot = ax[0].bar([0, 1], ber_rv.pmf([0, 1]))\n",
    "ax[0].set_xticks([0, 1])\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Mass\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "for i, bar in enumerate(bplot):\n",
    "  bar.set_color(\"crimson\") if i == 0 else bar.set_color(\"orange\")\n",
    "\n",
    "# The second one is a stair plot of the cumulative distribution function\n",
    "x = np.linspace(-1, 2, 100)\n",
    "ax[1].step(x, ber_rv.cdf(x))\n",
    "ax[1].set_xticks([-1, 0, 1, 2])\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.2. Binomial Distribution**\n",
    "![alt text](figuresA/image012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S = X_1 + X_2 + \\ldots + X_n$ where $n$ is a fixed integer and $X_1,\\ldots, X_n$ are __independent__ random variables that are distributed according to the Bernoulli distribution with the __same__ parameter $p$. Then $S$ will be a discrete random variable with a __Binomial Distribution__ with parameters $n$ and $p$, and we write $S\\sim \\mathrm{Binomial}(n;p)$. It has the probability mass function\n",
    "$$\n",
    "\\mathrm{Pr}(S = y) = \\mathrm{Binomial}(y \\vert n, p) := \\left(\\begin{array}{c} n\\\\y \\end{array} \\right) p^y(1-p)^{n-y}.\n",
    "$$\n",
    "\n",
    "Here\n",
    "$$\n",
    "\\left(\\begin{array}{c} n\\\\y \\end{array} \\right)  := \\frac{\n",
    "    n!\n",
    "}{\n",
    "    (n-y)!y!\n",
    "}\n",
    "$$\n",
    "is pronounced \"_n choose y_\" and is the total number of ways to take $y$ objects without replacement from a set of $n$ objects.\n",
    "\n",
    "The binomial distribution models the number of heads you get when flipping a coin $n$ times, where the probability of getting head in each flip is p. It could also model the probability distribution for how many cells will have a given phenotype when you measure $n$ total cells and the phenotype probability is $p$.\n",
    "\n",
    "* What is the mean of a Binomial random variable?\n",
    "* What is the variance and standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Binomial random variables\n",
    "from scipy.stats import binom\n",
    "\n",
    "n = 10  # number of trials - change this value to see the effect\n",
    "p = 0.7  # probability of success - change this value to see the effect\n",
    "\n",
    "# Create a Binomial random variable\n",
    "bin_rv = binom(n = n, p = p)\n",
    "\n",
    "# Make two plots to show the PMF and the CDF:\n",
    "# The first one is a bar chart of the probability mass function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "bplot = ax[0].bar(range(n+1), bin_rv.pmf(range(n+1)))\n",
    "ax[0].set_xticks(range(n+1))\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Mass\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "for i, bar in enumerate(bplot):\n",
    "  bar.set_color(\"crimson\")\n",
    "\n",
    "# The second one is a stair plot of the cumulative distribution function\n",
    "x = np.linspace(-1, n+1, 100)\n",
    "ax[1].step(x, bin_rv.cdf(x))\n",
    "ax[1].set_xticks(range(n+1))\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.3. Poisson Distribution**\n",
    "![alt text](figuresA/image013.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Poisson random variables\n",
    "from scipy.stats import poisson\n",
    "\n",
    "lambda_ = 5  # rate of events - change this value to see the effect\n",
    "\n",
    "# Create a Poisson random variable\n",
    "poi_rv = poisson(mu = lambda_)\n",
    "# Make two plots to show the PMF and the CDF:\n",
    "# The first one is a bar chart of the probability mass function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "bplot = ax[0].bar(range(20), poi_rv.pmf(range(20)))\n",
    "\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Mass\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "for i, bar in enumerate(bplot):\n",
    "  bar.set_color(\"crimson\")\n",
    "xticks = ax[0].get_xticks()\n",
    "\n",
    "# The second one is a stair plot of the cumulative distribution function\n",
    "x = np.arange(-1, 3*lambda_+1)\n",
    "ax[1].step(x+1, poi_rv.cdf(x))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.4. Geometric Distribution**\n",
    "![alt text](figuresA/image014.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Geometric random variables\n",
    "from scipy.stats import geom\n",
    "\n",
    "p = 0.4  # probability of success - change this value to see the effect\n",
    "\n",
    "# Create a Geometric random variable\n",
    "geo_rv = geom(p = p)\n",
    "\n",
    "# Make two plots to show the PMF and the CDF:\n",
    "# The first one is a bar chart of the probability mass function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "bplot = ax[0].bar(range(0, int(5/p)), geo_rv.pmf(range(0, int(5/p))))\n",
    "# ax[0].set_xticks(range(1, 11))\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Mass\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "for i, bar in enumerate(bplot):\n",
    "  bar.set_color(\"crimson\")\n",
    "xticks = ax[0].get_xticks()\n",
    "\n",
    "# The second one is a stair plot of the cumulative distribution function\n",
    "x = np.linspace(0, int(5/p), 100)\n",
    "ax[1].step(x, geo_rv.cdf(x))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.B. Common Continuous Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.B.1. Uniform Distribution**\n",
    "\n",
    "A continuous random variable $X$ is said to have a **uniform distribution** with parameters $a$ and $b$ (with $a< b$), denoted by $X\\sim U(a,b)$, if its probability density function is given by\n",
    "$$\n",
    "f_X(x) = \\frac{\n",
    "    1\n",
    "}{\n",
    "    b-a\n",
    "}\\mathbb{1}(a\\leq x \\leq b)\n",
    "$$\n",
    "where $\\mathbb{1}(A)=0$ if statement $A$ is false and $1$ otherwise. This distribution will be essential in implementing stochastic simulation algorithm and Metropolis-Hastings algorithm later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Uniform random variables\n",
    "from scipy.stats import uniform\n",
    "\n",
    "a = 0  # lower bound - change this value to see the effect\n",
    "b = 10  # upper bound - change this value to see the effect\n",
    "\n",
    "# Create a Uniform random variable\n",
    "uni_rv = uniform(loc = a, scale = b - a)\n",
    "\n",
    "# Make two plots to show the PDF and the CDF:\n",
    "# The first one is a line plot of the probability density function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "x = np.linspace(a-1, b+1, 100)\n",
    "ax[0].plot(x, uni_rv.pdf(x))\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Density\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "xticks = ax[0].get_xticks()\n",
    "\n",
    "# The second one is a line plot of the cumulative distribution function\n",
    "x = np.linspace(a-1, b+1, 100)\n",
    "ax[1].plot(x, uni_rv.cdf(x))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.B.2. Exponential Distribution**\n",
    "![alt text](figuresA/image017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution will be very important later in the course when we explore the wating times between stochastic events that describe gene regulatory changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Exponential random variables\n",
    "from scipy.stats import expon\n",
    "\n",
    "lambda_ = 0.25  # rate of events - change this value to see the effect\n",
    "\n",
    "# Create an Exponential random variable\n",
    "exp_rv = expon(scale = 1/lambda_)\n",
    "# Make two plots to show the PDF and the CDF:\n",
    "# The first one is a line plot of the probability density function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "x = np.linspace(-1, int(5/lambda_+1), 100)\n",
    "ax[0].plot(x, exp_rv.pdf(x))\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Density\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "xticks = ax[0].get_xticks()\n",
    "\n",
    "# The second one is a line plot of the cumulative distribution function\n",
    "x = np.linspace(-1, int(5/lambda_+1), 100)\n",
    "ax[1].plot(x, exp_rv.cdf(x))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.B.3. Gaussian Distribution**\n",
    "\n",
    "A continuous random variable $X$ has a normal distribution with parameters $\\mu$ and $\\sigma^2$, written $X\\sim N(\\mu, \\sigma^2)$ if it has probability density function\n",
    "$$\n",
    "f_X(x) = \\frac{\n",
    "    1\n",
    "}{\n",
    "    \\sqrt{2\\pi\\sigma^2}\n",
    "}\\exp\\left(-\\frac{\n",
    "    1\n",
    "}{\n",
    "    2\n",
    "}\\left(\\frac{\n",
    "    x-\\mu\n",
    "}{\n",
    "    \\sigma\n",
    "}\\right)^2\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "An important special case of the normal distribution is the standard normal distribution $N(0,1)$ which has parameters $\\mu=0$ and $\\sigma^2=1$.\n",
    "\n",
    "The normal distribution has a very important role in statistics. One reason is that many random quantitites tend to be approximately normally distributed for large sample size. Specifically, the Central Limit Theorem (CLT) states that for random variables $X_1,\\ldots, X_n$ that are independent and identically distributed with the same mean $\\mu$ and variance $\\sigma^2$, and let\n",
    "$$\n",
    "\\overline{X}:=\\frac{\n",
    "    X_1+\\ldots+X_n\n",
    "}{\n",
    "    n\n",
    "}$$\n",
    "be the sample mean, then the distribution of the random variable $$Z:=\\sqrt{n}\\left(\\frac{\n",
    "    \\overline{X} - \\mu\n",
    "}{\n",
    "    \\sigma\n",
    "}\\right)$$\n",
    "is well-approximated by a standard normal distribution $N(0,1)$ when $n$ is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using Normal random variables\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 0  # mean - change this value to see the effect\n",
    "sigma = 1  # standard deviation - change this value to see the effect\n",
    "\n",
    "# Create a Normal random variable\n",
    "norm_rv = norm(loc = mu, scale = sigma)\n",
    "\n",
    "# Make two plots to show the PDF and the CDF:\n",
    "# The first one is a line plot of the probability density function,\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "x = np.linspace(-mu-4*sigma, mu+4*sigma, 100)\n",
    "ax[0].plot(x, norm_rv.pdf(x))\n",
    "ax[0].set_xlabel(\"Outcome\")\n",
    "ax[0].set_ylabel(\"Probability Density\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "xticks = ax[0].get_xticks()\n",
    "\n",
    "# The second one is a line plot of the cumulative distribution function\n",
    "ax[1].plot(x, norm_rv.cdf(x))\n",
    "ax[1].set_xticks(xticks)\n",
    "ax[1].set_xlabel(\"Outcome\")\n",
    "ax[1].set_ylabel(\"Cumulative Probability\")\n",
    "sns.despine(offset=5, trim=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Sampling from distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.A. Sampling From Common Distributions using Numpy**\n",
    "\n",
    "Numpy provides a very nice random sampling library that is widely used throughout scientific computing.\n",
    "\n",
    "```np.random.____```\n",
    "\n",
    "| Method    | Description  |\n",
    "| ----------- | ----------- |\n",
    "| rand | uniform random from 0 to 1|\n",
    "| randn | univariate normal from a mean of 0 and a variance of 1 |\n",
    "| randint| random integers from low to high|\n",
    "| choice | pick a uniform random value from an array |\n",
    "| shuffle | shuffles an array along the first axis|\n",
    "\n",
    "Additionally np.random can sample from a list of statistical distributions, heres a short list:\n",
    "\n",
    "| Distribution    |\n",
    "| ----------- |\n",
    "| [beta](https://en.wikipedia.org/wiki/Beta_distribution) |\n",
    "| [gamma](https://en.wikipedia.org/wiki/Gamma_distribution) |\n",
    "| [lognormal](https://en.wikipedia.org/wiki/Log-normal_distribution)|\n",
    "| [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) |\n",
    "| [poisson](https://en.wikipedia.org/wiki/Poisson_distribution) |\n",
    "| [uniform](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) |  \n",
    "| [power](https://en.wikipedia.org/wiki/Power_law) |\n",
    "\n",
    "\n",
    "We can additionally set the rng state with np.random.seed(integer)\n",
    "\n",
    "| Method    | Description  |\n",
    "| ----------- | ----------- |\n",
    "| seed | set the seed of the RNG |\n",
    "| get_state | get the current state of the mersenne twister |\n",
    "\n",
    "---\n",
    "\n",
    "```Reading: Kinder, Nelson Section 6.2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some simple random data points and plotting them\n",
    "n = 96 #number of data points to sample\n",
    "\n",
    "randomNumberGenerator = 'randn' #e.g., \"rand\", \"randn\", \"randint\"\n",
    "examples_str = {'rand': rng.rand(2,n),\n",
    "            'randn': rng.randn(2,n),\n",
    "            'randint': rng.randint(0,1000,size = (2,n))}\n",
    "\n",
    "# Run the example string to generate the random numbers\n",
    "random_numbers = examples_str[randomNumberGenerator]\n",
    "\n",
    "# Plot the random numbers as 2D points\n",
    "plt.plot(random_numbers[0],random_numbers[1],'o' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some multivariate Normal (Gaussian) data points and plotting them\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "\n",
    "n = 674     # Number of data points to sample\n",
    "mean1 = 33  # Mean of the first dimension\n",
    "mean2 = 12  # Mean of the second dimension\n",
    "cov = rng.randn(2,2)*3      # Covariance matrix\n",
    "cov[0,0] = 2; cov[1,1] = 2;\n",
    "\n",
    "# Generate the data\n",
    "random_numbers = rng.multivariate_normal(np.array([mean1,mean2]), cov, size=n).T\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(random_numbers[0],random_numbers[1],'o' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histograms of log-normal data\n",
    "n = 781     # Number of data points to sample\n",
    "mean = 1.9  # Mean of the lognormal distribution\n",
    "sigma = 0.7 # Log-Standard deviation of the lognormal distribution\n",
    "\n",
    "# Generate the data\n",
    "random_numbers = rng.lognormal(mean,sigma, size=(n))\n",
    "\n",
    "# Plot the histogram of the data\n",
    "plt.hist(random_numbers,density=True,bins=40 )\n",
    "\n",
    "# Add the analytical LogNormal PDF to the plot\n",
    "x = np.linspace(0,max(random_numbers),100)\n",
    "pdf = np.exp(-(np.log(x)-mean)**2/(2*sigma**2))/(x*sigma*np.sqrt(2*np.pi))\n",
    "plt.plot(x,pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histogram of some Poisson data\n",
    "n = 200 # Number of data points to sample\n",
    "lam = 60 # Poisson parameter (mean)\n",
    "\n",
    "# Generate the data\n",
    "random_numbers = rng.poisson(lam, size=(n))\n",
    "\n",
    "print(f'The sample mean of the poisson data is {np.mean(random_numbers)}')\n",
    "print(f'The sample variance of the poisson data is {np.var(random_numbers)}')\n",
    "\n",
    "# Plot histogram of the data and compare it to the Poisson distribution\n",
    "plt.hist(random_numbers,density=True,bins=range(max(random_numbers)+1), align='mid')\n",
    "\n",
    "# Add the analytical Poisson distribution to the plot\n",
    "from scipy.stats import poisson\n",
    "x = range(max(random_numbers))\n",
    "plt.plot(x, poisson.pmf(x, lam), 'r-', ms=8, mec='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.B. Sampling arbitrary distributions using the Inverse Transform Sampling method.**\n",
    "![alt text](figuresA/image007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.1. Example for sampling from an arbitrary discrete distribution using the inverse transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for sampling from an arbitrary discrete distribution using the inverse transform method\n",
    "# Define the discrete distribution\n",
    "x = [1, 2, 3, 4, 5] # The outcomes\n",
    "p = [0.1, 0.2, 0.3, 0.2, 0.2] # The probabilities of the outcomes\n",
    "\n",
    "# Create the cumulative distribution function\n",
    "cdf = np.cumsum(p)\n",
    "\n",
    "# Sample from the distribution\n",
    "n = 1000\n",
    "samples = np.zeros(n)\n",
    "for i in range(n):\n",
    "    u = rng.rand()\n",
    "    samples[i] = x[np.argmax(cdf > u)]\n",
    "    \n",
    "# Plot the histogram of the samples\n",
    "plt.hist(samples,density=True,bins=range(1,7), align='left')\n",
    "\n",
    "# Add the analytical distribution to the plot\n",
    "plt.plot(x, p, 'r-', ms=8, mec='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B.2. Example for sampling from an arbitrary continuous distribution using the inverse transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for sampling from an arbitrary continuous distribution using the inverse transform method\n",
    "# Define the continuous distribution\n",
    "# Create a probability density function using sympy\n",
    "x = sym.symbols('x')\n",
    "lam = 0.5\n",
    "pdf_sym = lam*sym.exp(-lam*x) # The probability density function\n",
    "\n",
    "# Create the cumulative distribution function (by integrating the PDF from 0 to x)\n",
    "cdf_sym = sym.integrate(pdf_sym, (x, 0, x))\n",
    "\n",
    "# Create the inverse of the cumulative distribution function (by solving the relation \"u = CDF(x)\" for x)\n",
    "cdf_inv_sym = sym.solve(cdf_sym - sym.symbols('u'), x)\n",
    "\n",
    "print(f'The inverse of the CDF is {cdf_inv_sym}')\n",
    "\n",
    "# Create a function to convert uniform random numbers U(0,1) to samples from the distribution\n",
    "sample = sym.lambdify(sym.symbols('u'), cdf_inv_sym)\n",
    "\n",
    "# Sample from the distribution\n",
    "n = 1000\n",
    "samples = sample(rng.rand(n))\n",
    "\n",
    "# Plot the histogram of the samples\n",
    "plt.hist(samples,density=True,bins=40)\n",
    "\n",
    "# Add the analytical distribution to the plot\n",
    "x = np.linspace(0,20,100)\n",
    "pdf = lam*np.exp(-lam*x)\n",
    "plt.plot(x, pdf, 'r-', ms=8, mec='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Computing expectations from distributons and summary statistics from samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.A. Expectations of a random variable**\n",
    "![alt text](figuresA/image005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.B. Summary statistics from samples**\n",
    "![alt text](figuresA/image006.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for computing expected values (means and variances) from a discrete distribution\n",
    "\n",
    "# Try one of the following discrete distributions\n",
    "dist = binom(n = 20, p = 0.3) # Create a Poisson distribution with lambda = 5\n",
    "# dist = poisson(mu = 5) # Create a Poisson distribution with lambda = 5\n",
    "# dist = geom(p = 0.53) # Create a Poisson distribution with lambda = 5\n",
    "\n",
    "f = lambda x: x**1.3 # Create a function to compute the expected value of. E.g., f(x) = x^(1.3)\n",
    "distRange = np.arange(25) # Create a range of values from 0 to 19\n",
    "\n",
    "pdf = dist.pmf(distRange) # Compute the PMF of the distribution\n",
    "mean = np.sum(pdf*distRange) # Compute the mean of the distribution\n",
    "variance = np.sum(pdf*(distRange-mean)**2) # Compute the variance of the distribution    \n",
    "f_mean = np.sum(pdf*f(distRange)) # Compute the expected value of f(x)\n",
    "\n",
    "print(f'The computed mean of this distribution is {mean}')\n",
    "print(f'The computed variance of this distribution is {variance}')\n",
    "print(f'The computed mean of function f(x) is {f_mean}')\n",
    "\n",
    "# Compare to the built-in methods\n",
    "print(f'The mean of this distribution is {dist.mean()}')\n",
    "print(f'The variance of this distribution is {dist.var()}')\n",
    "\n",
    "# Now let's sample from the distribution and compute the sample mean and variance\n",
    "n = 1000 # Number of samples to draw\n",
    "samples = dist.rvs(size=n) # Draw n samples from the distribution   \n",
    "sampleMean = np.mean(samples) # Compute the sample mean \n",
    "sampleVariance = np.var(samples) # Compute the sample variance  \n",
    "samplefMean = np.mean(f(samples)) # Compute the sample mean of f(x)\n",
    "\n",
    "print(f'The sample mean of this distribution for {n} samples is {sampleMean}')\n",
    "print(f'The sample variance of this distribution for {n} samples is  {sampleVariance}')\n",
    "print(f'The sample mean of function f(x) for {n} samples is {samplefMean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for computing expected values (means and variances) from a continuous distribution\n",
    "\n",
    "# Try one of the following continuous distributions\n",
    "dist = uniform(loc = 0, scale = 10) # Create a Uniform distribution with a = 0 and b = 10\n",
    "# dist = expon(scale = 1/3) # Create a Exponential distribution with lambda = 3\n",
    "# dist = norm(loc = -3, scale = 2) # Create a Normal distribution with mean = -3 and standard deviation = 2 \n",
    "\n",
    "f = lambda x: x**4 # Create a function to compute the expected value of. E.g., f(x) = x^(4)\n",
    "\n",
    "distRange = np.linspace(-30,30,10000) # Create a range of values from 0 to 19\n",
    "stepSize = distRange[1]-distRange[0] # Compute the step size of the range\n",
    "\n",
    "pdf = dist.pdf(distRange) # Compute the PDF of the distribution\n",
    "mean = np.sum(pdf*distRange)*stepSize # Compute the mean of the distribution\n",
    "variance = np.sum(pdf*(distRange-mean)**2)*stepSize  # Compute the variance of the distribution    \n",
    "f_mean = np.sum(pdf*f(distRange))*stepSize # Compute the expected value of f(x)\n",
    "\n",
    "print(f'The computed mean of this distribution is {mean}')\n",
    "print(f'The computed variance of this distribution is {variance}')\n",
    "print(f'The computed mean of function f(x) is {f_mean}')\n",
    "\n",
    "# Compare to the built-in methods\n",
    "print(f'The mean of this distribution is {dist.mean()}')\n",
    "print(f'The variance of this distribution is {dist.var()}')\n",
    "\n",
    "# Now let's sample from the distribution and compute the sample mean and variance\n",
    "n = 1000 # Number of samples to draw\n",
    "samples = dist.rvs(size=n) # Draw n samples from the distribution   \n",
    "sampleMean = np.mean(samples) # Compute the sample mean \n",
    "sampleVariance = np.var(samples) # Compute the sample variance  \n",
    "samplefMean = np.mean(f(samples)) # Compute the sample mean of f(x)\n",
    "\n",
    "print(f'The sample mean of this distribution for {n} samples is {sampleMean}')\n",
    "print(f'The sample variance of this distribution for {n} samples is  {sampleVariance}')\n",
    "print(f'The sample mean of function f(x) for {n} samples is {samplefMean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Probability and Moment Generating Functions** (Optional)\n",
    "\n",
    "## **6.A. Probability Generating Functions** (Disrete Random Variables)\n",
    "A probability generating function (PGF) is a power series representation of the probability mass function of a discrete random variable. It is defined as\n",
    "$$\n",
    "G_X(z) = \\sum_{n=0}^{\\infty} p_X(n)z^n\n",
    "$$\n",
    "where $p_X(n)$ is the probability mass function of the random variable $X$. If we know the PGF of a random variable, we can calculate the probability of any event involving the random variable. For example, if we know $G_X(z)$, we can calculate the probability that $X$ is a specific value $n$ by taking the $n$-th derivative of $G_X(z)$ and evaluating it at $z=0$.\n",
    "\n",
    "Note, the z-transform used here is a discrete generalization of the Laplace transform. This transform is frequently used in signal processing and control theory.\n",
    "\n",
    "## **6.B. Moment Generating Functions** (Discrete Random Variables)\n",
    "The moment generating function (MGF) is a power series representation of the probability mass function of a discrete random variable. It is defined as\n",
    "$$\n",
    "M_X(t) = \\sum_{n=0}^{\\infty} p_X(n)e^{tn}\n",
    "$$\n",
    "where $p_X(n)$ is the probability mass function of the random variable $X$. The MGF is a useful tool for calculating the moments of a random variable. \n",
    "\n",
    "In the MGF, $t$ is a dummy variable. To get the $n$-th central moment of $X$, we can take the $n$-th derivative of the MGF and evaluate it at $t=0$.\n",
    "\n",
    "The relationship between the PGF and MGF is that the MGF is the PGF evaluated at $z=e^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.C. Examples of Discrete Distributions and their PGFs/MGFs**\n",
    "### **6.C.1. Example -- Bernoulli Distribution**\n",
    "the **Probability Generating Function (PGF)** of a **Bernoulli random variable** $X$ is given by\n",
    "$$\n",
    "G_X(z) = (1-\\theta) + \\theta z\n",
    "$$\n",
    "where $\\theta$ is the probability of success.\n",
    "\n",
    "Let's find the probability that $X$ is equal to 0 or 1 using the PGF:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Pr}(X=0) &= G_X(0) = 1-\\theta\\\\\n",
    "\\mathrm{Pr}(X=1) &= G_X(1) = \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows how the PGF can be used to calculate the probability of specific events involving the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Moment Generating Function (MGF)** of a **Bernoulli random variable** $X$ is found from the PGF by evaluating it at $z=e^t$:\n",
    "$$\n",
    "M_X(t) = (1-\\theta) + \\theta e^t\n",
    "$$\n",
    "The $n$-th central moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second central moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\theta\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\theta(1-\\theta) + \\theta^2\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows how the MGF can be used to calculate the moments of a random variable.\n",
    "\n",
    "**Bernoulli Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the PGF of Bernoulli random variables\n",
    "from sympy.abc import z\n",
    "t = sym.symbols('t')\n",
    "\n",
    "theta = 0.7 # Probability of success\n",
    "pgf = (1-theta) + theta*z  # The PGF of a Bernoulli random variable\n",
    "\n",
    "# Get the probability mass function from the PGF\n",
    "for i in range(2):\n",
    "    print(f'P(X = {i}) = {sym.diff(pgf,z,i).subs(z,0)}')\n",
    "\n",
    "# Compute the mean and variance of the Bernoulli random variable\n",
    "mgf = pgf.subs(z, sym.exp(t)) # The MGF of the Bernoulli random variable\n",
    "mean = sym.diff(mgf, t).subs(t,0) # Compute the mean\n",
    "secondMoment = sym.diff(mgf, t, 2).subs(t,0) # Compute the variance\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Bernoulli random variable is {mean}')\n",
    "print(f'The variance of the Bernoulli random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "ber_rv = bernoulli(p = theta)\n",
    "print(f'The (built in) mean of the Bernoulli random variable is {ber_rv.mean()}')\n",
    "print(f'The (built in) variance of the Bernoulli random variable is {ber_rv.var()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.C.2. Example -- Binomial Distribution**\n",
    "The **PGF** of a **binomial random variable** $X$ is given by\n",
    "$$\n",
    "G_X(z) = (1-\\theta + \\theta z)^n\n",
    "$$\n",
    "where $\\theta$ is the probability of success and $n$ is the number of trials.\n",
    "\n",
    "Let's find the probability that $X$ is equal to $k$ using the PGF:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Pr}(X=k) &= \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}G_X(z)\\right|_{z=0} = \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}(1-\\theta + \\theta z)^n\\right|_{z=0}\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows how the PGF can be used to calculate the probability of specific events involving the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MGF** of a **binomial random variable** $X$ is found from the PGF by evaluating it at $z=e^t$:\n",
    "$$\n",
    "M_X(t) = (1-\\theta + \\theta e^t)^n\n",
    "$$\n",
    "The $n$-th central moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second central moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = n\\theta\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = n\\theta(1-\\theta) + n\\theta^2\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows how the MGF can be used to calculate the moments of a random variable.\n",
    "\n",
    "**Binomial Distribtion Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the PGF of Binomial random variables\n",
    "from sympy.abc import z\n",
    "t = sym.symbols('t')\n",
    "\n",
    "n = 10 # Number of trials\n",
    "theta = 0.7 # Probability of success\n",
    "pgf = (1-theta + theta*z)**n  # The PGF of a Binomial random\n",
    "\n",
    "# Get the probability mass function from the PGF\n",
    "ProbMass = []\n",
    "for i in range(n+1):\n",
    "    pm = sym.diff(pgf,z,i).subs(z,0)/sym.factorial(i)\n",
    "    ProbMass.append(pm)\n",
    "\n",
    "# Plot the probability mass function and compare to the built-in method\n",
    "from scipy.stats import binom\n",
    "x = range(n+1)\n",
    "plt.bar(x, ProbMass)\n",
    "plt.plot(x, binom.pmf(x, n, theta), 'r-')\n",
    "plt.legend(['Analytical', 'Built-in'])\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance of the Binomial random variable\n",
    "mgf = pgf.subs(z, sym.exp(t)) # The MGF of the Binomial random variable\n",
    "mean = sym.diff(mgf, t).subs(t,0) # Compute the mean\n",
    "secondMoment = sym.diff(mgf, t, 2).subs(t,0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Binomial random variable is {mean}')\n",
    "print(f'The variance of the Binomial random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = binom(n = n, p = theta)\n",
    "print(f'The (builtin) mean of the Binomial random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Binomial random variable is {dist.var()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.C.3. Example -- Poisson Distribution**\n",
    "The **PGF** of a **Poisson random variable** $X$ is given by\n",
    "$$\n",
    "G_X(z) = e^{\\lambda(z-1)}\n",
    "$$\n",
    "where $\\lambda$ is the rate parameter.\n",
    "\n",
    "Let's find the probability that $X$ is equal to $k$ using the PGF:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Pr}(X=k) &= \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}G_X(z)\\right|_{z=0} = \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}e^{\\lambda(z-1)}\\right|_{z=0}\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows how the PGF can be used to calculate the probability of specific events involving the random variable.\n",
    "\n",
    "The **MGF** of a **Poisson random variable** $X$ is found from the PGF by evaluating it at $z=e^t$:\n",
    "$$\n",
    "M_X(t) = e^{\\lambda(e^t-1)}\n",
    "$$\n",
    "\n",
    "The $n$-th uncenterred moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second uncentered moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\lambda\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\lambda^2 + \\lambda\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Poisson Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the PGF of Poisson random variables\n",
    "from sympy.abc import z\n",
    "t = sym.symbols('t')\n",
    "\n",
    "lambda_ = 5 # Rate of events\n",
    "pgf = sym.exp(lambda_*(z-1))  # The PGF of a Poisson random variable\n",
    "\n",
    "# Get the probability mass function from the PGF\n",
    "ProbMass = []\n",
    "for i in range(20):\n",
    "    pm = sym.diff(pgf,z,i).subs(z,0)/sym.factorial(i)\n",
    "    ProbMass.append(pm)\n",
    "\n",
    "# Plot the probability mass function and compare to the built-in method\n",
    "from scipy.stats import poisson\n",
    "x = range(20)\n",
    "plt.bar(x, ProbMass)\n",
    "plt.plot(x, poisson.pmf(x, lambda_), 'r-')\n",
    "plt.legend(['Analytical', 'Built-in'])\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance of the Poisson random variable\n",
    "mgf = pgf.subs(z, sym.exp(t)) # The MGF of the Poisson random variable\n",
    "mean = sym.diff(mgf, t).subs(t,0) # Compute the mean\n",
    "secondMoment = sym.diff(mgf, t, 2).subs(t,0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Poisson random variable is {mean}')\n",
    "print(f'The variance of the Poisson random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = poisson(mu = lambda_)\n",
    "print(f'The (builtin) mean of the Poisson random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Poisson random variable is {dist.var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.C.4. Example -- Geometric Distribution**\n",
    "The **PGF** of a **geometric random variable** $X$ is given by\n",
    "$$\n",
    "G_X(z) = \\frac{\\theta z}{1-(1-\\theta)z}\n",
    "$$\n",
    "where $\\theta$ is the probability of success.  \n",
    "\n",
    "Please note -- in this formulation, the support of the geometric distribution is $k=0,1,2,\\ldots$. \n",
    "\n",
    "Let's find the probability that $X$ is equal to $k$ using the PGF:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Pr}(X=k) &= \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}G_X(z)\\right|_{z=0} = \\left.\\frac{1}{k!}\\frac{d^k}{dz^k}\\frac{\\theta z}{1-(1-\\theta)z}\\right|_{z=0}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The **MGF** of a **geometric random variable** $X$ is found from the PGF by evaluating it at $z=e^t$:\n",
    "$$\n",
    "M_X(t) = \\frac{\\theta e^t}{1-(1-\\theta)e^t}\n",
    "$$\n",
    "\n",
    "The $n$-th uncenterred moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second uncentered moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\frac{1}{\\theta}\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\frac{2-\\theta}{\\theta^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Geometric Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the PGF of Geometric random variables\n",
    "from sympy.abc import z\n",
    "t = sym.symbols('t')\n",
    "\n",
    "p = 0.4 # Probability of success\n",
    "pgf = p*z/(1-(1-p)*z)  # The PGF of a Geometric random variable\n",
    "\n",
    "# Get the probability mass function from the PGF\n",
    "ProbMass = [sym.diff(pgf,z,i).subs(z,0)/sym.factorial(i) for i in range(20)]\n",
    "\n",
    "# Plot the probability mass function and compare to the built-in method\n",
    "from scipy.stats import geom\n",
    "x = range(20)\n",
    "plt.bar(x, ProbMass)\n",
    "plt.plot(x, geom.pmf(x, p), 'r-')  \n",
    "plt.legend(['Analytical', 'Built-in'])\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance of the Geometric random variable\n",
    "mgf = pgf.subs(z, sym.exp(t)) # The MGF of the Geometric random variable\n",
    "mean = sym.diff(mgf, t).subs(t,0) # Compute the mean\n",
    "secondMoment = sym.diff(mgf, t, 2).subs(t,0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Geometric random variable is {mean}')\n",
    "print(f'The variance of the Geometric random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = geom(p = p)\n",
    "print(f'The (builtin) mean of the Geometric random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Geometric random variable is {dist.var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.D. Characteristic Functions and Moment Generating Functions (Continuous Random Variables)**\n",
    "For continuous random variables, we can also define a **Characteristic Function** and in many cases **moment generating function** (MGF) to provide a power series representation of the probability density function. \n",
    "\n",
    "The **Characteristic Function** is defined as\n",
    "$$\n",
    "\\phi_X(t) = \\int_{-\\infty}^{\\infty} f_X(x)e^{itx}dx\n",
    "$$\n",
    "where $f_X(x)$ is the probability density function of the random variable $X$. The characteristic function is a useful tool for calculating the moments of a random variable, and you might recognize that it is essentially the Fourier transform of the probability density function (but where the convention is to use the dummy variable $t$ instead of $\\omega$).\n",
    "\n",
    "The **Moment Generating Function** (MGF) is defined as\n",
    "$$\n",
    "M_X(t) = \\int_{-\\infty}^{\\infty} f_X(x)e^{tx}dx\n",
    "$$\n",
    "where $f_X(x)$ is the probability density function of the random variable $X$. The MGF is a useful tool for calculating the moments of a random variable, and you might recognize that it is essentially the Laplace transform of the probability density function (but where the convention is to use $t$ instead of $s$).\n",
    "\n",
    "In the CF and MGF, $t$ is a dummy variable. To get the $n$-th un-centered moment of $X$, we can take the $n$-th derivative of the MGF and evaluate it at $t=0$.\n",
    "\n",
    "### **6.D.1. Example -- Uniform Distribution**\n",
    "The **CF** and **MGF** of a **uniform random variable** $X$ are given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_X(t) &= \\frac{e^{tb}-e^{ta}}{it(b-a)}\\\\\n",
    "M_X(t) &= \\frac{e^{tb}-e^{ta}}{t(b-a)}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $a$ and $b$ are the parameters of the distribution.\n",
    "\n",
    "The $n$-th uncentered moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second uncentered moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\frac{b-a}{2}\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\frac{b^2+ab+a^2}{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Uniform Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the MGF of Uniform random variables\n",
    "a = 0 # Lower bound\n",
    "b = 10 # Upper bound\n",
    "t = sym.symbols('t')\n",
    "\n",
    "cf = sym.exp(b*t) - sym.exp(a*t) # The characteristic function of a Uniform random variable\n",
    "mgf = (sym.exp(b*t) - sym.exp(a*t))/(t*(b-a)) # The MGF of a Uniform random variable\n",
    "\n",
    "# Compute the mean and variance of the Uniform random variable\n",
    "# Note, here we need to compute the limit of the MGF as t approaches 0, since the MGF is \n",
    "# not defined at t = 0\n",
    "mean = sym.limit(sym.diff(mgf, t), t, 0)\n",
    "secondMoment = sym.limit(sym.diff(mgf, t, 2),t,0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Uniform random variable is {mean}')\n",
    "print(f'The variance of the Uniform random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = uniform(loc = a, scale = b-a)\n",
    "print(f'The (builtin) mean of the Uniform random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Uniform random variable is {dist.var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.D.2. Example -- Exponential Distribution**\n",
    "The **CF** and **MGF** of an **exponential random variable** $X$ are given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_X(t) &= \\frac{\\lambda}{\\lambda - it}\\\\\n",
    "M_X(t) &= \\frac{\\lambda}{\\lambda - t}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\lambda$ is the rate parameter.\n",
    "\n",
    "The $n$-th uncentered moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second uncentered moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\frac{1}{\\lambda}\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\frac{2}{\\lambda^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Exponential Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the CF and MGF of Exponential random variables\n",
    "lambda_ = 0.25 # Rate of events\n",
    "t = sym.symbols('t')\n",
    "\n",
    "cf = 1/(1 - 1j*t/lambda_) # The characteristic function of an Exponential random variable\n",
    "mgf = lambda_/(lambda_ - t) # The MGF of an Exponential random variable\n",
    "\n",
    "# Compute the mean and variance of the Exponential random variable\n",
    "mean = sym.limit(sym.diff(mgf, t), t, 0) # Compute the mean\n",
    "secondMoment = sym.limit(sym.diff(mgf, t, 2), t, 0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Exponential random variable is {mean}')\n",
    "print(f'The variance of the Exponential random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = expon(scale = 1/lambda_)\n",
    "print(f'The (builtin) mean of the Exponential random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Exponential random variable is {dist.var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.D.3. Example -- Gaussian Distribution**\n",
    "The **CF** and **MGF** of a **Gaussian random variable** $X$ are given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_X(t) &= e^{it\\mu - \\frac{1}{2}\\sigma^2t^2}\\\\\n",
    "M_X(t) &= e^{it\\mu + \\frac{1}{2}\\sigma^2t^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma^2$ is the variance.\n",
    "\n",
    "The $n$-th uncentered moment of $X$ can be found by taking the $n$-th derivative of the MGF and evaluating it at $t=0$. Let's find the first and second uncentered moments of $X$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[X] &= M_X'(0) = \\mu\\\\\n",
    "\\mathrm{E}[X^2] &= M_X''(0) = \\mu^2 + \\sigma^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Gaussian Distribution Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the CF and MGF of Normal random variables\n",
    "mu = 0 # Mean\n",
    "sigma = 1 # Standard deviation\n",
    "t = sym.symbols('t')\n",
    "\n",
    "cf = sym.exp(1j*mu*t - 0.5*sigma**2*t**2) # The characteristic function of a Normal random variable\n",
    "mgf = sym.exp(mu*t + 0.5*sigma**2*t**2) # The MGF of a Normal random variable\n",
    "\n",
    "# Compute the mean and variance of the Normal random variable\n",
    "mean = sym.diff(mgf, t).subs(t,0) # Compute the mean\n",
    "secondMoment = sym.diff(mgf, t, 2).subs(t,0)\n",
    "variance = secondMoment - mean**2 # Compute the variance\n",
    "\n",
    "print(f'The mean of the Normal random variable is {mean}')\n",
    "print(f'The variance of the Normal random variable is {variance}')\n",
    "\n",
    "# Check against the built-in methods\n",
    "dist = norm(loc = mu, scale = sigma)\n",
    "print(f'The (builtin) mean of the Normal random variable is {dist.mean()}')\n",
    "print(f'The (builtin) variance of the Normal random variable is {dist.var()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.E. Sum of Independent Random Variables**\n",
    "If $X$ and $Y$ are independent random variables with probability distributions $f_X(x)$ and $f_Y(y)$, then the probability distribution of the sum $Z=X+Y$ is given by the convolution of the two distributions:\n",
    "$$\n",
    "f_Z(z) = \\int_{-\\infty}^{\\infty} f_X(x)f_Y(z-x)dx\n",
    "$$\n",
    "Convolutions can be difficult to calculate in general, but if you recall from the Fourier transform, the convolution of two functions in real space is equivalent to the product of their Fourier transforms in Fourier space. This means that if you know the characteristic functions of $X$ and $Y$, you can calculate the characteristic function of $Z$ by multiplying the characteristic functions of $X$ and $Y$. Let's do an example for two Gaussian random variables.\n",
    "\n",
    "### **6.E.1. Example -- Sum of Gaussian Random Variables**\n",
    "If $X\\sim N(\\mu_1, \\sigma_1^2)$ and $Y\\sim N(\\mu_2, \\sigma_2^2)$ are independent Gaussian random variables, then the **CF** and **MGF** of $Z$ can be found by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_Z(t) &= \\phi_X(t)\\phi_Y(t) = e^{it\\mu_1 - \\frac{1}{2}\\sigma_1^2t^2}e^{it\\mu_2 - \\frac{1}{2}\\sigma_2^2t^2} = e^{it(\\mu_1+\\mu_2) - \\frac{1}{2}(\\sigma_1^2+\\sigma_2^2)t^2}\\\\\n",
    "M_Z(t) &= M_X(t)M_Y(t) = e^{it\\mu_1 + \\frac{1}{2}\\sigma_1^2t^2}e^{it\\mu_2 + \\frac{1}{2}\\sigma_2^2t^2} = e^{it(\\mu_1+\\mu_2) + \\frac{1}{2}(\\sigma_1^2+\\sigma_2^2)t^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "This shows that the sum $Z=X+Y$ is also Gaussian with mean $\\mu_1+\\mu_2$ and variance $\\sigma_1^2+\\sigma_2^2$!\n",
    "\n",
    "\n",
    "**Sum of Gaussian Random Variables Example in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the convolution of two Gaussian distributions (to find the distribution of \n",
    "# the sum of two Gaussian random variables).\n",
    "from sympy.abc import x, k\n",
    "y = sym.symbols('y')\n",
    "\n",
    "mu1 = 3 # Mean of the first Gaussian\n",
    "sigma1 = 1 # Standard deviation of the first Gaussian\n",
    "mu2 = 2 # Mean of the second Gaussian\n",
    "sigma2 = 2 # Standard deviation of the second Gaussian\n",
    "\n",
    "# Create the CFs of the two Gaussian random variables\n",
    "cf1 = sym.exp(1j*mu1*x - 0.5*sigma1**2*x**2) # The CF of the first Gaussian\n",
    "cf2 = sym.exp(1j*mu2*x - 0.5*sigma2**2*x**2) # The CF of the second Gaussian\n",
    "\n",
    "# Compute the CF of the sum of the two Gaussian random variables\n",
    "cf_sum = cf1*cf2\n",
    "# Simplify the expression\n",
    "cf_sum = sym.simplify(cf_sum)\n",
    "\n",
    "# Create a Gaussian with mean mu1+mu2 and variance sigma1^2+sigma2^2\n",
    "cf_sum_gaussian = sym.exp(1j*(mu1+mu2)*x - 0.5*(sigma1**2+sigma2**2)*x**2)\n",
    "\n",
    "# Compare the computed CF to the CF of the Gaussian\n",
    "print(f'The computed characteristic function is {cf_sum}')\n",
    "print(f'The characteristic function of the Gaussian is {cf_sum_gaussian}')\n",
    "print(f'The difference between the two is {sym.simplify(cf_sum - cf_sum_gaussian)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for the MGF of the sum of two Gaussian random variables\n",
    "from sympy.abc import x, k\n",
    "y = sym.symbols('y')\n",
    "\n",
    "mu1 = 3 # Mean of the first Gaussian\n",
    "sigma1 = 1 # Standard deviation of the first Gaussian\n",
    "mu2 = 2 # Mean of the second Gaussian\n",
    "sigma2 = 2 # Standard deviation of the\n",
    "\n",
    "# Create the MGFs of the two Gaussian random variables\n",
    "mgf1 = sym.exp(mu1*x + 0.5*sigma1**2*x**2) # The MGF of the first Gaussian\n",
    "mgf2 = sym.exp(mu2*x + 0.5*sigma2**2*x**2) # The MGF of the second Gaussian\n",
    "\n",
    "# Compute the MGF of the sum of the two Gaussian random variables\n",
    "mgf_sum = mgf1*mgf2\n",
    "# Simplify the expression\n",
    "mgf_sum = sym.simplify(mgf_sum)\n",
    "\n",
    "# Create a Gaussian with mean mu1+mu2 and variance sigma1^2+sigma2^2\n",
    "mgf_sum_gaussian = sym.exp((mu1+mu2)*x + 0.5*(sigma1**2+sigma2**2)*x**2)\n",
    "\n",
    "# Compare the computed MGF to the MGF of the Gaussian\n",
    "print(f'The computed moment generating function is {mgf_sum}')\n",
    "print(f'The moment generating function of the Gaussian is {mgf_sum_gaussian}')\n",
    "\n",
    "# Compute the means from the computed MGFs\n",
    "mean1 = sym.diff(mgf1, x).subs(x,0)\n",
    "mean2 = sym.diff(mgf2, x).subs(x,0)\n",
    "mean_sum = sym.diff(mgf_sum, x).subs(x,0)\n",
    "mean_sum_gaussian = sym.diff(mgf_sum_gaussian, x).subs(x,0)\n",
    "\n",
    "print(f'The computed mean of the first Gaussian is {mean1}')\n",
    "print(f'The computed mean of the second Gaussian is {mean2}')\n",
    "print(f'The computed mean of the sum is {mean_sum}')\n",
    "\n",
    "# Compare the computed means to the actual means\n",
    "print(f'The mean of the first Gaussian is {mu1}')\n",
    "print(f'The mean of the second Gaussian is {mu2}')\n",
    "print(f'The mean of the sum is {mu1 + mu2}')\n",
    "\n",
    "# Compute the variances from the computed MGFs\n",
    "var1 = sym.diff(mgf1, x, 2).subs(x,0) - mean1**2\n",
    "var2 = sym.diff(mgf2, x, 2).subs(x,0) - mean2**2\n",
    "var_sum = sym.diff(mgf_sum, x, 2).subs(x,0) - mean_sum**2\n",
    "var_sum_gaussian = sym.diff(mgf_sum_gaussian, x, 2).subs(x,0) - mean_sum_gaussian**2\n",
    "\n",
    "print(f'The computed variance of the first Gaussian is {var1}')\n",
    "print(f'The computed variance of the second Gaussian is {var2}')\n",
    "print(f'The computed variance of the sum is {var_sum}')\n",
    "\n",
    "# Compare the computed variances to the actual variances\n",
    "print(f'The variance of the first Gaussian is {sigma1**2}')\n",
    "print(f'The variance of the second Gaussian is {sigma2**2}')\n",
    "print(f'The variance of the sum is {sigma1**2 + sigma2**2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.F. Central Limit Theorem**\n",
    "The **Central Limit Theorem** (CLT) states that the sum of a large number of independent and identically distributed random variables will be approximately normally distributed. This is a very powerful result that allows us to make inferences about the distribution of a sum of random variables even if we don't know the distribution of the individual random variables. The Central Limit Theorem is the reason why the normal distribution is so important in statistics.\n",
    "\n",
    "Let's explore the idea of the CLT through the analysis of the characteristic function of the sum of random variables. \n",
    "\n",
    "Suppose that $X_1, X_2, \\ldots, X_n$ are independent and identically distributed random variables with mean $\\mu$ and variance $\\sigma^2$.  Because we know the mean and variance of the random variables, we know that the first two terms of the characteristic function expansion are given by\n",
    "$$\n",
    "\\phi_{X_i}(t) = e^{it\\mu - \\frac{1}{2}\\sigma^2t^2 + o(t^2)} = e^{it\\mu - \\frac{1}{2}\\sigma^2t^2}e^{o(t^2)}\n",
    "$$\n",
    "where $o(t^2)$ is a term that goes to zero faster than $t^2$ as $t\\rightarrow 0$.  The characteristic function of the sum of the random variables is then given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi_{S_n}(t) &= \\prod_{i=1}^n \\phi_{X_i}(t)\\\\\n",
    "&= \\left(e^{it\\mu - \\frac{1}{2}\\sigma^2t^2}e^{o(t^2)}\\right)^n\\\\\n",
    "&= e^{itn\\mu - \\frac{1}{2}n\\sigma^2t^2}\\left(e^{o(t^2)}\\right)^n\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Now, the term $o(t^2)$ will go to zero faster than $t^2$ as $t\\rightarrow 0$, and further taking this to the power $n$ will make it go to zero even faster.  This shows that the characteristic function of the sum of the random variables is converging to that of a Gaussian with mean $n\\mu$ and variance $n\\sigma^2$.  This is the essence of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to illustrate the central limit theorem\n",
    "# Create a distribution to sample from\n",
    "dist = uniform(loc = 0, scale = 10)\n",
    "# Other options to try (uncomment one at a time, or add your own):\n",
    "# dist = expon(scale = 1/3)\n",
    "# dist = norm(loc = -3, scale = 2)\n",
    "\n",
    "# Get mean and variance of the distribution\n",
    "mean = dist.mean()\n",
    "variance = dist.var()\n",
    "\n",
    "# Sample from the distribution\n",
    "nSets = 10000\n",
    "nSetSize = 100\n",
    "samples = dist.rvs(size=(nSetSize, nSets))\n",
    "\n",
    "# Compute the sample means\n",
    "sampleMeans = np.mean(samples, axis=0)\n",
    "\n",
    "# Plot the histogram of the sample means\n",
    "plt.hist(sampleMeans, bins=80, density=True)\n",
    "\n",
    "# Add the analytical distribution to the plot\n",
    "x = np.linspace(mean-4*np.sqrt(variance/nSetSize), mean+4*np.sqrt(variance/nSetSize), 100)\n",
    "pdf = norm.pdf(x, loc=mean, scale=np.sqrt(variance/nSetSize))\n",
    "plt.plot(x, pdf, 'r-')\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(['CLT Approximation','Sampled'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
