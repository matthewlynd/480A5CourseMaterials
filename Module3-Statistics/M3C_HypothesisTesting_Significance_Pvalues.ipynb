{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <summary></summary>\n",
    "         <div> <p></p> </div>\n",
    "         <div style=\"font-size: 20px; width: 800px;\"> \n",
    "              <h1>\n",
    "               <left>Intro to Basic Probability and Statistics: Hypothesis Testing, Significance, P-Values.</left>\n",
    "              </h1>\n",
    "              <p><left>============================================================================</left> </p>\n",
    "<pre>Course: BIOM 480A5, Spring 2025\n",
    "Instructor: Brian Munsky\n",
    "Authors: Huy Vo, Ania Baetica, Brian Munsky\n",
    "Contact Info: munsky@colostate.edu\n",
    "</pre>\n",
    "         </div>\n",
    "    </p>\n",
    "\n",
    "</html>\n",
    "\n",
    "<details>\n",
    "  <summary>Copyright info</summary>\n",
    "\n",
    "```\n",
    "Copyright 2024 Brian Munsky\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "\n",
    "2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "\n",
    "3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "```\n",
    "<details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "Upon completion of this lesson, you should be able to:\n",
    "- Describe the concepts and approaches of **Hypothesis Testing**\n",
    "- Define what is meant by **Significance**\n",
    "- Describe the meaning and use of **P-Values**\n",
    "- Describe and Perform Student’s t-test\n",
    "- Describe and Perform Pairwise Student’s t-test\n",
    "- Describe and Perform Willcox Rank-Sum test\n",
    "- Describe and Perform Willcox Signed-Rank test\n",
    "- Describe and Perform ANOVA test for variations in multiple groups\n",
    "- Describe and Perform Tukey’s HSD test for pairwise comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "\n",
    "# %pip install statsmodels   \n",
    "# %pip install seaborn\n",
    "# If you don't have them, uncomment the lines above to install statsmodels and seaborn\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the user is usig the currect working directory\n",
    "import os\n",
    "path = os.getcwd()\n",
    "# Get the folder name\n",
    "folder = os.path.basename(path)\n",
    "\n",
    "print('Checking current directory...')\n",
    "if folder == 'Module3-Statistics':\n",
    "    print(f'You are currently in the correct directory: {folder}')\n",
    "elif folder == '480A5CourseMaterials':\n",
    "    os.chdir('Module3-Statistics')\n",
    "    print('Changed to Module3-Statistics directory')\n",
    "else:\n",
    "    print('You are in the wrong directory. Navigate to the Module3-Statistics directory and try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "In this lesson, we will introduce the concept of hypothesis testing, significance, and p-values. We will also introduce several statistical tests that are commonly used in biology and bioinformatics. These tests include the Student’s t-test, the Wilcoxon rank-sum test, the Wilcoxon signed-rank test, the ANOVA test, and Tukey’s HSD test. We will discuss the assumptions of these tests, how to perform them, and how to interpret the results.\n",
    "\n",
    "But first, we need to generate some data to work with.\n",
    "\n",
    "## **1.A. Example Data Sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data Sets\n",
    "# In this notebook, we are going to use several different pairs of data sets to illustrate\n",
    "# different hypothesis tests.\n",
    "\n",
    "# The first two data sets are unpaired sets of random numbers drawn from a NORMAL distribution\n",
    "# with different means but the same standard deviations.\n",
    "Nnormal = [100,120]\n",
    "normal0 = rng.normal(0, 1, Nnormal[0])\n",
    "normal1 = rng.normal(0.5, 1, Nnormal[1])\n",
    "\n",
    "# The next pair are paired data sets, where each pair of numbers is drawn from a NORMAL distribution,\n",
    "# but the second number in each pair is drawn from a normal distribution with a mean that is\n",
    "# different than the mean of the first number in the pair. The standard deviation of the two\n",
    "# distributions is the same.\n",
    "Npaired = 100\n",
    "parametersPatients = rng.normal(10, 0.5, Npaired) # \n",
    "effect = 0.2\n",
    "NormalPairBefore = rng.normal(parametersPatients, 1) \n",
    "NormalPairAfter = rng.normal(parametersPatients+effect, 1) \n",
    "\n",
    "# The next pair of data sets are unpaired data sets drawn from a LOGNORMAL distribution with the\n",
    "# same standard deviation and the same means.\n",
    "Nlognormal = [100,120]\n",
    "lognormal0 = rng.lognormal(0, 1, Nlognormal[0])\n",
    "lognormal1 = rng.lognormal(0, 1, Nlognormal[1])\n",
    "\n",
    "# The next pair are paired data sets, where each pair of numbers is drawn from a LOGNORMAL distribution,\n",
    "# but the second number in each pair is drawn from a lognormal distribution with a mean that is\n",
    "# different than the mean of the first number in the pair. The standard deviation of the two\n",
    "# distributions is the same.\n",
    "Npaired = 100\n",
    "parametersPatients = rng.normal(0, 0.5, Npaired) \n",
    "effect = 0.2\n",
    "LogNPairBefore = rng.lognormal(np.log(parametersPatients), 1)\n",
    "LogNPairAfter = rng.lognormal(np.log(parametersPatients+effect), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Hypothesis testing**\n",
    "\n",
    "![alt text](figuresC/image.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.A. The Testing Process**\n",
    "\n",
    "![alt text](figuresC/image.002.png)\n",
    "\n",
    "## **2.B. Interpreting Results**\n",
    "\n",
    "The outcome of a hypothesis test provides insights into the plausibility of the null hypothesis. A rejection of the null hypothesis suggests evidence in favor of the alternative hypothesis, while a failure to reject implies insufficient evidence to warrant rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.C. Significance and P-Values**\n",
    "\n",
    "Hypothesis testing involves two critical concepts that serve as guiding beacons in our statistical journey: **significance** and **p-values**. These concepts provide us with quantitative measures to evaluate the strength of evidence in support of our hypotheses and to make informed decisions based on our analyses.\n",
    "\n",
    "### **2.C.1. Significance: Separating Signal from Noise**\n",
    "\n",
    "![alt text](figuresC/image.003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.C.2. Test Statistics: Quantifying Differences**\n",
    "\n",
    "![alt text](figuresC/image.004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.C.3. P-Values: Assessing Evidence**\n",
    "\n",
    "![alt text](figuresC/image.005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.C.4. Interpreting P-Values in the Context of Significance**\n",
    "\n",
    "![alt text](figuresC/image.006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.C.5. Significance, P-Values, and Decision Making**\n",
    "\n",
    "Significance, p-values, and test statistics play pivotal roles in the decision-making process during hypothesis testing. By setting a significance level ($\\alpha$) beforehand, we establish a threshold for rejecting the null hypothesis. If the calculated p-value falls below this threshold, and the test statistic exceeds critical values, we deem the results statistically significant and infer that the observed effect is unlikely to occur due to chance alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Parametric and Non-Parametric Tests** \n",
    "\n",
    "**Parametric tests** make assumptions about the distribution of the data, typically assuming that the data follow a specific probability distribution, such as the normal distribution (see t-test below.)\n",
    "\n",
    "**Non-parametric tests**, on the other hand, do not make assumptions about the distribution of the data or make fewer assumptions compared to parametric tests (see Wilcox signed rank test below). They are used when the data do not meet the assumptions of parametric tests or when the data are ordinal or non-normally distributed.\n",
    "\n",
    "Here are some key differences between parametric and non-parametric tests:\n",
    "\n",
    "**Assumptions**: Parametric tests make specific assumptions about the population distribution, such as normality and homogeneity of variances, while non-parametric tests make fewer or no assumptions about the distribution of the data.\n",
    "\n",
    "**Type of Data**: Parametric tests are often used for interval or ratio scale data that are normally distributed, while non-parametric tests are more flexible and can be used with ordinal, interval, or ratio scale data, as well as non-normally distributed data.\n",
    "\n",
    "**Test Statistics**: Parametric tests use test statistics that are based on the parameters of the population distribution (e.g., mean, standard deviation), while non-parametric tests use test statistics that are based on the ranks or order of the data.\n",
    "\n",
    "**Power**: Parametric tests tend to have higher statistical power (ability to detect true differences or effects) when the assumptions are met, while non-parametric tests may have lower power but are more robust to violations of assumptions.\n",
    "\n",
    "In summary, the choice between parametric and non-parametric tests depends on the nature of the data, the assumptions that can be made about the data, and the specific research question being addressed. If the assumptions of parametric tests are met, they are generally preferred because they can provide more powerful statistical tests. However, if the assumptions are violated, non-parametric tests may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.A. Parametric Test Example -- Student's T-Test**\n",
    "![alt text](FiguresC/image.006B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **Student's t-test** is a **parametric statistical test** used to determine **is there is a significant difference between the means of two groups**. The t-test is commonly used in scientific research, especially in fields such as psychology, medicine, and biology, where it is used to compare means from experiments. For example, it often used to assess the effectiveness of interventions or treatments.\n",
    "\n",
    "It was developed by **William Sealy Gosset**, who published it **under the pseudonym \"Student\" in 1908** while working at the Guinness Brewery in Dublin, Ireland. \n",
    "\n",
    "The t-test is particularly useful when working with **small sample sizes** or **when the population standard deviation is unknown**.\n",
    "\n",
    "The formula for the t-test depends on whether the samples being compared are independent or dependent (paired)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.1. Independent Samples t-test**\n",
    "For independent samples, the t-statistic is computed by:\n",
    "$$\n",
    "t = \\frac{{\\bar{X}_1 - \\bar{X}_2}}{{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\bar{X}_1 \\text{ and } \\bar{X}_2 \\text{ are the sample means of the two groups}, \\\\\n",
    "    & n_1 \\text{ and } n_2 \\text{ are the sample sizes of the two groups}, \\\\\n",
    "    & s_p \\text{ is the pooled standard deviation, given by:} \\\\\n",
    "    & s_p = \\sqrt{\\frac{{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}}{{n_1 + n_2 - 2}}}, \\\\\n",
    "    & \\text{where } s_1 \\text{ and } s_2 \\text{ are the standard deviations of the two groups}.\n",
    "\\end{align*}\n",
    "\n",
    "Under the **Null Hypothesis** that the means of the two groups are equal, the t-statistic would follow a **t-distribution** with $n_1+n_2−2$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Student's t Distribution\n",
    "\n",
    "# Make a figure plotting the Student's t distribution. Shade the area under the curve\n",
    "# that corresponds to the significance level alpha. Also, plot a vertical line at the\n",
    "# observed t-statistic value. The title of the plot should be the title parameter.\n",
    "def plot_t_distribution(ax, alpha, observed_value, title=' ', sided='two-sided'):\n",
    "    # Create a range of x values from -4 to 4\n",
    "    x = np.linspace(-4, 4, 1000)\n",
    "    # Create a t-distribution with df degrees of freedom\n",
    "    t = stats.t.pdf(x, 5)\n",
    "    # Plot the t-distribution\n",
    "    ax.plot(x, t, 'b', label='t-distribution')\n",
    "    # Shade the area under the curve that corresponds to the significance level alpha\n",
    "    if sided == 'two-sided':\n",
    "        ax.fill_between(x, 0, t, where=(x > stats.t.ppf(1-alpha/2, 5)) | (x < stats.t.ppf(alpha/2, 5)), color='b', alpha=0.3, label='reject H0 region')\n",
    "    elif sided == 'less':\n",
    "        ax.fill_between(x, 0, t, where=(x < stats.t.ppf(alpha, 5)), color='b', alpha=0.3, label='reject H0 region')\n",
    "    elif sided == 'greater':\n",
    "        ax.fill_between(x, 0, t, where=(x > stats.t.ppf(1-alpha, 5)), color='b', alpha=0.3, label='reject H0 region')\n",
    "\n",
    "    # Plot a vertical line at the observed t-statistic value\n",
    "    ax.axvline(observed_value, color='r', linestyle='--', label='observed t-statistic')\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "    # Set the title of the plot\n",
    "    ax.set_xlabel('t-statistic')\n",
    "    ax.set_ylabel('P(t)')\n",
    "    ax.set_title(title)\n",
    "   \n",
    "t_statistic = -3.1\n",
    "significance_level = 0.05\n",
    "sided = 'less'\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "plot_t_distribution(ax, significance_level, t_statistic, sided=sided, \\\n",
    "                    title=f't-distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.A.2. P-values for the Independent Samples t-test:**\n",
    "Under the null hypothesis that the two distributions have the same mean, the t-statistic would be close to zero. The P-value using the t-test is found by calculating the probability that the a random sample drawn from from a t-distributon would be at least as large/small as the calculated t-statistic.  \n",
    "\n",
    "The probability that a larger $t$ would be sampled by chance is found by calculating **survival function** for the absolute value of $t$ (i.e., $\\text{surv}(|t|) = 1-CDF(|t|)$). \n",
    "\n",
    "*Two-Sided Test*: In a two-sided test, the null hypothesis typically states that there is no difference between the means of the two related groups, while the alternative hypothesis states that there is a difference, without specifying the direction of the difference. The two-sided test is used when you want to determine if there is a significant difference between the means in either direction. In this case, the critical region is split between the two tails of the distribution, and the p-value that at last as large a difference would be observed in the mean of the two populations is determined as $PV(t) = 2*\\text{surv}(|t|)$.\n",
    "\n",
    "*One-Sided Test*: In a one-sided test, the null hypothesis states that there is no difference or that group 2 is greater than or equal to group 1, while the alternative hypothesis specifies that group 1 is greater than group 2. The one-sided test is used when you are specifically interested in whether one group is greater than the other, but not if it's smaller. The critical region is located entirely in one tail of the distribution, and the p-value for testing if is determined as $PV(t) = \\text{surv}(t)$.\n",
    "\n",
    "### **3.A.3. Important limitations of the Independent Samples t-test:**\n",
    "\n",
    "Like any test, the t-test has important limitations that one should consider before applying the test:\n",
    "\n",
    "1) **Assumption of Normality**: The t-test assumes that the data in each group are normally distributed. If this assumption is violated, the results of the test may be invalid.\n",
    "\n",
    "2) **Sensitive to Outliers**: The t-test can be sensitive to outliers, particularly when sample sizes are small.\n",
    "\n",
    "3) **Equal Variances Assumption**: The traditional independent samples t-test assumes that the variances of the two groups are equal. If this assumption is violated, alternative versions of the t-test, such as Welch's t-test, should be used.\n",
    "\n",
    "4) **Sample Size**: The t-test may not perform well with very small sample sizes, as the t-distribution approaches the normal distribution only as sample size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.4. Examples of One- and Two-Sided t-tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the Independent Student's t-test\n",
    "\n",
    "# Define a function for the t-test\n",
    "def ttest(data0, data1, alternative='two-sided' ):\n",
    "    # Compute the number of samples in each group\n",
    "    N = [len(data0), len(data1)]\n",
    "    Ndof = N[0] + N[1] - 2 # number of degrees of freedom\n",
    "    \n",
    "    # Compute the t-statistic\n",
    "    sp = np.sqrt(((N[0]-1)*np.var(data0) + (N[1]-1)*np.var(data1)) / (N[0]+N[1]-2))\n",
    "    t = (np.mean(data0) - np.mean(data1)) / (sp * np.sqrt(1/N[0] + 1/N[1]))\n",
    "    \n",
    "    # Compute the p-value for 1- and 2-sided t-test.\n",
    "    if alternative=='two-sided':\n",
    "        pv = 2*stats.t.sf(np.abs(t), Ndof)\n",
    "    elif alternative=='greater':\n",
    "        pv = stats.t.sf(t, Ndof)\n",
    "    elif alternative=='less':\n",
    "        pv = stats.t.sf(-t, Ndof)\n",
    "   \n",
    "    return t, pv\n",
    "\n",
    "# Compute the t-test\n",
    "alternative = 'two-sided'  # 'two-sided', 'greater', or 'less'\n",
    "t, pv = ttest(normal0, normal1, alternative=alternative)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print('t-test using our code: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Compare to built in t-test function:\n",
    "t_statistic, p_value = stats.ttest_ind(normal0, normal1, alternative=alternative)\n",
    "print('t-test using built in code: t =', t, 'p-value =', pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Test Results to Visualize Significance\n",
    "\n",
    "Below are examples of various pots to visualize samples and their significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the t-distribution and the observed t-statistic\n",
    "\n",
    "# Here, we plot the t-distribution with the observed t-statistic and the significance level\n",
    "# shaded. The title of the plot is the title parameter.  This give some insight into the\n",
    "# interpretation of the p-value, but is not typically included in a publication or report.\n",
    "\n",
    "degrees_of_freedom = len(normal0) + len(normal1) - 2\n",
    "significance_level = 0.05\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "plot_t_distribution(ax, significance_level, t_statistic, sided=alternative, \\\n",
    "                    title=f't-distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Box Plots and showing Significance using Seaborn\n",
    "import pandas as pd\n",
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Put normal0 and normal1 into a pandas DataFrame\n",
    "df1 = pd.DataFrame({'Normal 0': normal0})  # Create a DataFrame with normal0\n",
    "df2 = pd.DataFrame({'Normal 1': normal1})  # Create a DataFrame with normal1\n",
    "df = pd.concat([df1, df2], axis=1) # Concatenate the two DataFrames (note, axis=1 means concatenate along the columns)\n",
    "\n",
    "def add_signif_bar(ax, x0, x1, p_value, bar_col='k', linewidth=1.2, fontsize=12, significance_levels=[0.05, 0.01, 0.001]):\n",
    "    # Add significance annotations to the plot\n",
    "    # Get the y-axis limits\n",
    "    ylim = ax.get_ylim()\n",
    "    edge = (ylim[1] - ylim[0]) * 0.02\n",
    "    bary = ylim[1] + (ylim[1] - ylim[0]) * 0.1\n",
    "    ax.plot([x0,x0,x1,x1], [bary-edge, bary, bary, bary-edge], lw=linewidth, c=bar_col)\n",
    "    ax.set_ylim([ylim[0], ylim[1] + (ylim[1] - ylim[0]) * 0.2])\n",
    "    # Add significance annotations to the plot\n",
    "    if p_value > significance_levels[0]: \n",
    "        significance_level = 'n.s.'\n",
    "    else:\n",
    "        significance_level = ''\n",
    "        for i, alpha in enumerate(significance_levels):\n",
    "            if p_value < alpha:\n",
    "                significance_level += '*' \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    ax.text((x0+x1)/2, bary, significance_level, ha='center', va='bottom', c='k')\n",
    "\n",
    "    print('Significance Levels:')\n",
    "    for i, significance_level in  enumerate(significance_levels):\n",
    "        stars = '*'*(i+1)\n",
    "        print(f'{stars} = {significance_level}')\n",
    "\n",
    "\n",
    "# Create figures with three subplots\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Create a boxplot of the data using seaborn \n",
    "ax[0] = sns.boxplot(data=df, ax=ax[0])\n",
    "ax[0].set_title('Boxplot of Normal 0 and Normal 1')  # Set the title of the plot \n",
    "ax[0].set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "add_signif_bar(ax[0], 0, 1, p_value)\n",
    "\n",
    "# Create a boxenplot of the data using seaborn \n",
    "ax[1] = sns.boxenplot(data=df, ax=ax[1])\n",
    "ax[1].set_title('Boxenplot of Normal 0 and Normal 1')  # Set the title of the plot \n",
    "ax[1].set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "add_signif_bar(ax[1], 0, 1, p_value)\n",
    "\n",
    "# Create a violin of the data using seaborn \n",
    "ax[2] = sns.violinplot(data=df, ax=ax[2])\n",
    "ax[2].set_title('Violin of Normal 0 and Normal 1')  # Set the title of the plot \n",
    "ax[2].set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "add_signif_bar(ax[2], 0, 1, p_value)\n",
    "\n",
    "# Create a pointplot of the data using seaborn\n",
    "ax[3] = sns.swarmplot(data=df, ax=ax[3])\n",
    "ax[3].set_title('Swarm plot of Normal 0 and Normal 1')  # Set the title of the plot \n",
    "ax[3].set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "add_signif_bar(ax[3], 0, 1, p_value, significance_levels=[0.005, 0.001, 0.0001])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.A.5. An aside on the one sided and two sided tests.** \n",
    "\n",
    "Consider the strange situation for two data sets, where the t-statistic is 2.2. The positive t-statistic arises when the mean of the first group (Group0) is larger than that of the second group (Group1).  We can plot the t-distribution as we did above with as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2 panel figures, one with two-sided and one with one-sided test:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "plot_t_distribution(ax[0], significance_level, 2.2, sided='two-sided', \\\n",
    "                    title=f't-distribution (two-sided)')\n",
    "plot_t_distribution(ax[1], significance_level, 2.2, sided='greater', \\\n",
    "                    title=f't-distribution (one-sided)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice on the left, that we do **not** have sufficient evidence to reject the null hypothesis that the *two means are equal*.  We could have a t-statistic whose absolute value is 2.2 by chance alone more than 5% of the time if the null hypothesis were true (i.e., if the two means were equal).\n",
    "\n",
    "But on the right, we **do** have sufficient evidence to reject the null hypothesis that the mean of Group 1 is greater than or equal than that for Group 0. We would observe a t-statistic whose value is 2.2 by chance alone less than 5% of the time if the null hypothesis were true (i.e., if the mean of Group 1 were greater than or equal to that of Group 0).\n",
    "\n",
    "This seems like a paradox and can lead to a lot of confusion.  \n",
    "\n",
    "Essentially, the reason for this seeming paradox is that in the two sided test, the null hypothesis (i.e., that the two means are exactly equal), needs to be rejected if the t-statistic is either very large **or** very small. Therefore, the threshold for rejecting the null hypothesis must be higher in the two sided test than in the one sided test.\n",
    "\n",
    "Read more about this \"paradox\" here:\n",
    "\n",
    "https://www.onesided.org/articles/the-paradox-of-one-sided-v-two-sided-tests-of-significance.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.B. Paired Values t-test**\n",
    "\n",
    "The **paired t-test** is a statistical test used to determine whether there is a significant difference between the means of two related groups. It's typically used when the samples being compared are not independent but instead have some sort of natural pairing or matching. For example, the paired t-test might be used to compare the effectiveness of a medication before and after treatment in the same group of patients.\n",
    "\n",
    "Here's how the paired t-test works:\n",
    "\n",
    "**Data Collection**: You collect data from two related groups, where each observation in one group is paired with a corresponding observation in the other group. For example, if you're comparing measurements before and after giving a hypothetical medication, each patient's measurement before treatment is paired with their measurement after treatment.\n",
    "\n",
    "**Calculate Differences**: For each pair of observations, calculate the difference between the two measurements. You'll end up with a new set of differences.\n",
    "\n",
    "**Calculate Sample Mean and Standard Deviation of Differences**: Calculate the mean ($d$) and standard deviation ($\\sigma_d$) of these differences.\n",
    "\n",
    "**Compute the t-statistic**: Use the formula for the one-sample t-test to calculate the t-statistic:\n",
    "\n",
    "$$ t=\\frac{d}{\\sigma_d/\\sqrt{n}} $$\n",
    "\n",
    "Where:\n",
    "* $d$ is the mean of the differences.\n",
    "* $\\sigma_d$​ is the standard deviation of the differences.\n",
    "* $n$ is the number of pairs.\n",
    "\n",
    "**Determine the p-value**: Use the t-distribution with $n-1$ degrees of freedom to determine the p-value associated with the calculated t-statistic.\n",
    "\n",
    "**Limitations**\n",
    "* The differences between pairs should be normally distributed.\n",
    "* The paired differences should be independent of each other.\n",
    "\n",
    "### **3.B.1. Example of Paired t-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Illustrate of the Paired t-test\n",
    "# Generate some paired data\n",
    "def pairedTTest(data0,data1,alternative='two-sided'):\n",
    "    # Compute the number of samples in each group\n",
    "    N = len(data0)\n",
    "    Ndof = N - 1 # number of degrees of freedom\n",
    "    \n",
    "    # Compute the t-statistic\n",
    "    mu = np.mean(data0-data1)\n",
    "    sig = np.std(data0-data1)\n",
    "    t = mu / (sig / np.sqrt(N))\n",
    "    \n",
    "    # Compute the p-value for the 1- and 2-sided test:\n",
    "    if alternative=='two-sided':\n",
    "        pv = 2*stats.t.sf(np.abs(t), Ndof)\n",
    "    elif alternative=='greater':\n",
    "        pv = stats.t.sf(t, Ndof)\n",
    "    elif alternative=='less':\n",
    "        pv = stats.t.sf(-t, Ndof)\n",
    "    return t, pv    \n",
    "\n",
    "N = len(NormalPairBefore) # number of patients\n",
    "\n",
    "# Compute the t-test\n",
    "alternative = 'two-sided'  # 'two-sided', 'greater', or 'less'\n",
    "t, pv = ttest(NormalPairBefore, NormalPairAfter, alternative=alternative)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print('t-test using our code: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Compare to built in t-test function:\n",
    "t_statistic, p_value = stats.ttest_rel(NormalPairBefore, NormalPairAfter, alternative=alternative)\n",
    "print('t-test using built in code: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Plot the t-distribution and the observed t-statistic using our previous function\n",
    "degrees_of_freedom = N - 1\n",
    "significance_level = 0.05\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "plot_t_distribution(ax[0], significance_level, t_statistic, sided=alternative, \\\n",
    "                    title=f't-distribution')\n",
    "\n",
    "# Plot the data and the significance using seaborn\n",
    "df1 = pd.DataFrame({'Before': NormalPairBefore})  # Create a DataFrame with NormalPairBefore\n",
    "df2 = pd.DataFrame({'After': NormalPairAfter})  # Create a DataFrame with NormalPairAfter\n",
    "df = pd.concat([df1, df2], axis=1) # Concatenate the two DataFrames (note, axis=1 means concatenate along the columns)\n",
    "\n",
    "# Create a boxenplot \n",
    "ax[1] = sns.boxenplot(data=df)\n",
    "ax[1].set_title('Boxplot of Pair 0 and Pair 1')  # Set the title of the plot \n",
    "ax[1].set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "add_signif_bar(ax[1], 0, 1, pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Non-Parametric Tests**\n",
    "Non-parametric tests are statistical tests that do not make assumptions about the distribution of the data. They are used when the data do not meet the assumptions of parametric tests or when the data are ordinal or non-normally distributed. Non-parametric tests are often used in situations where the data are skewed, have outliers, or are not normally distributed.\n",
    "\n",
    "Some of the most common non-parametric tests include the Wilcoxon rank-sum test, the Wilcoxon signed-rank test, the Kolmogorov-Smirnov test, and the Kruskal-Wallis test. These tests are used to compare groups, test for differences between distributions, and assess relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **4.A. Wilcox Ranked Sum Test (a.k.a, the Mann-Whitney U Test)**\n",
    "\n",
    "This **Wilcox Ranked Sum** test is used to compare the **medians** of two independent groups. This **non-parametric test** is typically used when the assumptions of the independent samples (needed for the t-test above) are not met, such as when the data are not normally distributed or when the variances are not equal.\n",
    "\n",
    "Here's a brief overview of the Wilcoxon rank-sum test (Mann-Whitney U test):\n",
    "\n",
    "**Purpose**: The Wilcoxon rank-sum test is used to determine whether the distributions of two independent groups differ significantly in terms of their medians.\n",
    "\n",
    "**Assumptions**: The main assumption of the Wilcoxon rank-sum test is that the two groups are independent and that the data within each group are at least ordinal (that is there is some natural ranking for the data, i.g., $a<b<c<c<d,\\ldots $).\n",
    "\n",
    "**Null Hypotheses**.  Like the t-test, the Wilcox Ranked Sum can be evaluated as a **two-sided** or **one-sided** test.  The corresponding Null-Hypotheses are:     \n",
    "* **Two-Sided Test Null Hypothesis**:  The medians of the two independent groups are equal.\n",
    "* **Left Tailed Test Null Hypothesis**: The median of group 1 is greater than or equal to the median of group 2.\n",
    "* **Right Tailed Test Null Hypothesis**: The median of group 1 is less than or equal to the median of group 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.A.1. Wilcox Ranked Sum Test Procedure, Interpretation, and Limitations**:\n",
    "1) Combine the data from the two groups into a single dataset.\n",
    "2) Rank all the observations in the combined dataset from smallest to largest, ignoring the group labels.\n",
    "3) Calculate the sum of ranks for each group.\n",
    "4) Use the smaller of the two sums of ranks as the test statistic (often denoted as U).\n",
    "5) Compare the test statistic to critical values from the Wilcoxon rank-sum distribution or calculate a p-value.\n",
    "\n",
    "**Interpretation**: If the test statistic is significantly different from what would be expected by chance, it suggests that the distributions of the two groups differ significantly in terms of their medians.\n",
    "\n",
    "**Limitations**\n",
    "The Wilcoxon rank-sum test is a non-parametric alternative to the independent samples t-test and is particularly useful when the assumptions of the t-test are not met. It does not assume normality or equal variances and is based on the ranks of the observations rather than their actual values.  However, like all tests, it has limitations:\n",
    "\n",
    "* **Assumption of Independence**: The Wilcoxon rank-sum test assumes that the observations within each group are independent of each other and that the two groups are independent of each other. Violations of this assumption can lead to inaccurate results.\n",
    "\n",
    "* **Ordinal Data Requirement**: While the Wilcoxon rank-sum test does not require the data to be normally distributed, it does require that the data be at least ordinal. If the data are purely nominal (e.g., categories without a natural order), the test may not be appropriate.\n",
    "\n",
    "* **Sensitivity to Ties**: The Wilcoxon rank-sum test can be sensitive to ties (i.e., identical values) in the data, especially if there are many ties or if the ties are not evenly distributed between the groups. This can affect the accuracy of the test results.\n",
    "\n",
    "* **Less Power Than the t-test**: In general, parametric tests such as the independent samples t-test tend to have more statistical power (i.e., are better at detecting true differences) than non-parametric tests like the Wilcoxon rank-sum test, especially when the assumptions of the parametric tests are met.\n",
    "\n",
    "* **Difficulty in Interpreting Effect Size**: While the Wilcoxon rank-sum test can determine whether there is a statistically significant difference between the groups, it does not provide a direct measure of effect size. Interpreting the practical significance of the results can therefore be more challenging compared to parametric tests.\n",
    "\n",
    "## **4.A.2. Example for Performing the Wilcox Ranked Sum test for unpaired data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's apply the Wilcoxon Ranked Sum Test to our original data sets\n",
    "# Compute the Wilcoxon ranked sum test\n",
    "t, pv = stats.ranksums(normal0, normal1)\n",
    "print('Wilcoxon ranked sum test: t =', t, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.A.2. Example for Performing the Wilcox Ranked Sum test for Paired data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's apply the Wilcoxon Ranked Sum Test to our paired data sets\n",
    "# Compute the Wilcoxon ranked sum test\n",
    "t, pv = stats.wilcoxon(NormalPairBefore, NormalPairAfter)\n",
    "print('Wilcoxon ranked sum test: t =', t, 'p-value =', pv)\n",
    "\n",
    "# Let's see how that compares to the t-test\n",
    "t, pv = pairedTTest(NormalPairBefore, NormalPairAfter)\n",
    "print('t-test: t =', t, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.B. Komogorov Smirnov (KS) Test**\n",
    "\n",
    "The **Kolmogorov-Smirnov test**, often abbreviated as the **KS test**, is another non-parametric test after mathematicians Andrey Kolmogorov and Nikolai Smirnov. It was first introduced by Kolmogorov in 1933 and later refined by Smirnov in 1948. Originally developed to test goodness-of-fit for continuous probability distributions (we will use it for this a lot later int he course), the KS test has since found widespread application in various fields, including statistics, physics, biology, and finance.\n",
    "\n",
    "As a **non-parametric** test, meaning it makes no assumptions about the underlying probability distribution of the data. Unlike parametric tests, which rely on specific distributional assumptions, the KS test is more flexible and can be applied to data with unknown distributions or distributions that deviate from normality.\n",
    "\n",
    "**Specific Null Hypotheses:**\n",
    "The KS test is designed to test whether a sample comes from a specific continuous distribution, or to compare two samples to determine if they are drawn from the same distribution. The specific null hypotheses tested by the KS test depend on the context:\n",
    "\n",
    "* **Goodness-of-Fit Test**: The null hypothesis for a goodness-of-fit test is that the sample data are drawn from a specified theoretical distribution.\n",
    "\n",
    "* **Two-Sample Test**: The null hypothesis for a two-sample KS test is that the two samples are drawn from the same continuous distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.B.1. The KS Statistic** \n",
    "\n",
    "The test statistic, denoted as $D$, is computed as the maximum absolute difference between the empirical cumulative distribution functions (ECDFs) of the two samples (for a two-sample test) or between the ECDF of the sample and CDF of the theoretical distribution (for a one-sample test). Mathematically, it is calculated as follows:\n",
    "\n",
    "$$D_{\\text{2-sample}}= {\\rm max}_i \\left| F_1(x_i)−F_2(x_i) \\right|$$\n",
    "$$D_{\\text{1-sample}}= {\\rm max}_j \\left| F_1(x_j)−F(x_j) \\right|$$\n",
    "where $F_1(x_i)$ and $F_2(x_i)$ are the empirical CDFs of the two samples at the $i^{\\rm th}$ observation, $F(x_i)$ is the CDF of the theoretical distribution at the $i^{\\rm th}$  observation, $x_i$ and $y_j$ are the ordered values of the samples and the theoretical distribution, respectively.\n",
    "\n",
    "## **4.B.2. Calculating P-Values for the KS Test**:\n",
    "Once the test statistic $D$ is computed, the p-value is obtained by comparing it to critical values from the Kolmogorov distribution (for one-sample tests) or the limiting distribution (for large sample sizes in two-sample tests) or by simulation or tabulation (for small sample sizes in two-sample tests). The specific distribution of the test statistic under the null hypothesis depends on the sample size, denoted by $n$, and whether it's a one-sample or two-sample test.\n",
    "\n",
    "* **P-Values for One-Sample Test**:\n",
    "    \n",
    "    For a one-sample KS test (where we compare a sample to a specified theoretical distribution), the distribution of the test statistic $D$ is approximated by the **Kolmogorov distribution**, which is derived from the properties of the Kolmogorov-Smirnov statistic. The Kolmogorov distribution's cumulative distribution function (CDF) is complex and cannot be represented by a simple mathematical formula. Instead, critical values or p-values are typically obtained from tables or computational algorithms.\n",
    "\n",
    "* **P-Values for Two-Sample Test**:\n",
    "    \n",
    "    For a two-sample KS test, when comparing two samples to determine if they are drawn from the same distribution, the distribution of the test statistic $D$ depends on the sample sizes $n_1$​ and $n_2$​. When both sample sizes are large (usually $n_1,n_2>35$), the distribution of $D$ can be approximated by the limiting distribution derived by Smirnov. However, for smaller sample sizes, the distribution of $D$ can be complex and is typically determined empirically through simulation or tabulation.\n",
    "\n",
    "For **two-sided** tests, where the alternative hypothesis is that the samples are not drawn from the same distribution, the p-value is computed as the probability of observing a test statistic greater than or equal to the observed value, plus the probability of observing a test statistic smaller than or equal to the negative of the observed value. \n",
    "\n",
    "For **one-sided** tests, the p-value is calculated using only one of these probabilities, depending on the direction of the alternative hypothesis.\n",
    "\n",
    "In summary, the Kolmogorov-Smirnov test is a powerful tool in statistical analysis, particularly useful when distributional assumptions are unknown or not met. Its versatility and simplicity make it a valuable asset in hypothesis testing and model validation across a wide range of disciplines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.B.3. Python Example of KS Test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for the KS Test.\n",
    "\n",
    "# Make a function called \"KS_illustrate\" to compute and plot the cumnulative distributions for two empirical data sets.\n",
    "# The function should also compute the KS statistic, and draw a vertical line at the location\n",
    "# of the KS statistic on the plot between the two CDFs. The title of the plot should be the\n",
    "# title parameter. The function should return the KS statistic and the p-value for the KS test.\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "def KS_illustrate(data0, data1, title=' ', significance_levels=[0.05, 0.01, 0.001]):\n",
    "    # Compute the empirical CDFs\n",
    "    ecdf0 = ECDF(data0)\n",
    "    ecdf1 = ECDF(data1)\n",
    "    \n",
    "    # Create a range of x values from -4 to 4\n",
    "    x = np.linspace(min(data0.min(),data1.min()), max(data0.max(),data1.max()), 1000)\n",
    "    # Compute the CDFs for the two data sets\n",
    "    y0 = ecdf0(x)\n",
    "    y1 = ecdf1(x)\n",
    "    # Compute the KS statistic\n",
    "    ks = np.max(np.abs(y0 - y1))\n",
    "    # Compute the p-value for the KS test\n",
    "    pv = stats.ks_2samp(data0, data1)\n",
    "    # Create a figure and axis\n",
    "    ax = plt\n",
    "    # Plot the CDFs for the two data sets\n",
    "    ax.plot(x, y0, 'b', label='ECDF-0')\n",
    "    ax.plot(x, y1, 'r', label='ECDF-1')\n",
    "    # Draw a vertical line at the location of the KS statistic\n",
    "    ax.plot([ks,ks],[ECDF(data0)(ks),ECDF(data1)(ks)],'g--',label='KS statistic' )\n",
    "    # Draw a horizontal line at the height of the KS statistic\n",
    "    ax.plot([min(data0.min(),data1.min()),max(data0.max(),data1.max())],[ECDF(data0)(ks),ECDF(data0)(ks)],'g--')\n",
    "    ax.plot([min(data0.min(),data1.min()),max(data0.max(),data1.max())],[ECDF(data1)(ks),ECDF(data1)(ks)],'g--')\n",
    "\n",
    "    # make a plot of the limiting distribution of the KS statistic in a new plot\n",
    "    ax.plot(x, stats.kstwobign.cdf(x, len(data0), len(data1)), 'k--', label='Limiting Distribution')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "    # Set the title of the plot\n",
    "    ax.title(title)\n",
    "\n",
    "    # Add significance annotations to the plot\n",
    "    if pv.pvalue > significance_levels[0]: \n",
    "        significance_level = 'n.s.'\n",
    "    else:\n",
    "        significance_level = ''\n",
    "        for i, alpha in enumerate(significance_levels):\n",
    "            if pv.pvalue < alpha:\n",
    "                significance_level += '*' \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    xlim = ax.xlim()\n",
    "    ysign = (ECDF(data0)(ks)+ECDF(data1)(ks))*0.475\n",
    "    ax.text(xlim[1]-0.1*(xlim[1]-xlim[0]), ysign, significance_level, ha='center', va='bottom', c='k')\n",
    "\n",
    "    print('Significance Levels:')\n",
    "    for i, significance_level in  enumerate(significance_levels):\n",
    "        stars = '*'*(i+1)\n",
    "        print(f'{stars} = {significance_level}')\n",
    "\n",
    "    return ks, pv\n",
    "\n",
    "# Compute the KS statistic and p-value for the KS test\n",
    "ks, pv = KS_illustrate(normal0, normal1, title='Empirical CDFs and KS Statistic')\n",
    "\n",
    "# Or if we just want to compute using builtin function:\n",
    "ks_statistic, p_value = stats.ks_2samp(normal0, normal1)\n",
    "print('KS test using built in code: KS =', ks_statistic, 'p-value =', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. ANOVA (ANalysis Of VAriance)**\n",
    "## **5.A. Introduction to ANOVA**\n",
    "\n",
    "![alt text](figuresC/image.007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.B. Steps for Performing the ANOVA**\n",
    "![alt text](figuresC/image.008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.C. Example of the ANOVA test in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our own (Unoptimized) Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a data set on which to perform ANOVA\n",
    "N = 100\n",
    "Ngroups = 3\n",
    "group0 = rng.normal(0, 1, N)\n",
    "group1 = rng.normal(0.01, 1, N)\n",
    "group2 = rng.normal(1, 1, N)\n",
    "\n",
    "# Running the ANOVA test without the use of the built-in function\n",
    "# compute the f statistic\n",
    "N = [len(group0), len(group1), len(group2)]\n",
    "Ngroups = len(N)\n",
    "Ndata = N[0] + N[1] + N[2]\n",
    "data = np.concatenate((group0, group1, group2))\n",
    "\n",
    "# Compute the group means and overall means.\n",
    "grandMean = np.mean(data)\n",
    "groupMean = [np.mean(group0), np.mean(group1), np.mean(group2)]\n",
    "\n",
    "# Compute the group variances\n",
    "groupVar = [np.var(group0), np.var(group1), np.var(group2)]\n",
    "\n",
    "# Compute the within group mean square differences\n",
    "dfwithin = Ndata - Ngroups\n",
    "SSW = np.dot(N, groupVar)\n",
    "MSwithin = SSW / dfwithin\n",
    "\n",
    "# Compute the between group mean square differences\n",
    "dfbetween = Ngroups - 1\n",
    "SSB = np.dot(N, (groupMean - grandMean)**2)\n",
    "MSbetween = SSB / dfbetween\n",
    "\n",
    "# Comptue teh F-statistic\n",
    "F = MSbetween / MSwithin\n",
    "\n",
    "# Compute the p-value using the F-distribution\n",
    "pv = 1 - stats.f.cdf(F, dfbetween, dfwithin)\n",
    "print('ANOVA (Custom): F =', F, 'p-value =', pv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Builtin Stats routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets compare our result to the built-in ANOVA function\n",
    "F, pv = stats.f_oneway(group0, group1, group2)\n",
    "print('ANOVA (Built in): F =', F, 'p-value =', pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Tukey Honestly Significant Difference Test**\n",
    "\n",
    "Upon rejecting the null hypothesis in the ANOVA test, we have evidence that the groups do not all have the same mean, but we do not yet know which groups are significantly different from which other groups. And, when there are lots of groups (e.g., lots of different drugs being tested), this may not be a trivial concern.\n",
    "\n",
    "The Tukey HSD test is a pairwise comparison test, and it is used to determine which groups are different from which other groups. The test is based on the studentized range distribution, and it is used to determine the minimum difference between group means that is statistically significant.  The more groups that there are the more likely it is that some pairs will be different by mere chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the Tukey Honestly Significant Difference (HSD) Test\n",
    "\n",
    "#mLet's generate some data to use for the Tukey HSD test\n",
    "N = 100\n",
    "Ngroups = 3\n",
    "means = np.zeros(Ngroups)\n",
    "stds = np.ones(Ngroups)\n",
    "\n",
    "# Change the means for the last two groups\n",
    "means[-2:] = [0.5,0.75]\n",
    "\n",
    "groups = [rng.normal(means[i], stds[i], N) for i in range(Ngroups)]\n",
    "\n",
    "from statsmodels.stats.multicomp import MultiComparison \n",
    "from statsmodels.stats.libqsturng import qsturng\n",
    "\n",
    "# Create the data\n",
    "data = np.concatenate(groups)\n",
    "groupLabels = np.concatenate([[i]*N for i in range(Ngroups)])\n",
    "\n",
    "# Create a MultiComparison object\n",
    "mc = MultiComparison(data, groupLabels)\n",
    "\n",
    "# Compute the Tukey HSD test\n",
    "result = mc.tukeyhsd()\n",
    "print(result)\n",
    "# Print the p-values\n",
    "print(result.pvalues)\n",
    "\n",
    "# Now go back and change the number of groups and see how the p-values change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Violin Plots to Visualize the Data and Significance\n",
    "# Create a pandas DataFrame with each group in a separate column\n",
    "dflist = [] # Create an empty list to store the DataFrames\n",
    "for i in range(Ngroups):\n",
    "    dflist.append(pd.DataFrame({f'Group {i}': groups[i]}))  # Create a DataFrame for each group and append it to the list\n",
    "df = pd.concat(dflist, axis=1)  # Concatenate the list of DataFrames into a single DataFrame\n",
    "\n",
    "# Create a violin plot of the data using seaborn\n",
    "ax = sns.violinplot(data=df)\n",
    "ax.set_title('Violin plot of Group 0, Group 1, and Group 2')  # Set the title of the plot\n",
    "ax.set_ylabel('Value')  # Set the y-axis label of the plot\n",
    "\n",
    "# Add significance annotations to the plot\n",
    "add_signif_bar(ax, 0, 1, result.pvalues[0])\n",
    "add_signif_bar(ax, 0, 2, result.pvalues[1])\n",
    "add_signif_bar(ax, 1, 2, result.pvalues[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
